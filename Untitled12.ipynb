{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYs+KvajacVm+hF3N41DGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhineetbhardwaj/Bharat-intern/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZShWl61688Z",
        "outputId": "013f0282-8490-45ee-8473-8c82f725fe52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.891466612870688\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       835\n",
            "           1       0.52      0.17      0.26       290\n",
            "           2       0.91      0.96      0.94      3832\n",
            "\n",
            "    accuracy                           0.89      4957\n",
            "   macro avg       0.75      0.65      0.67      4957\n",
            "weighted avg       0.88      0.89      0.88      4957\n",
            "\n",
            "{'prediction': 1, 'rating': 0.6630989310130573, 'flag': False}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/HateSpeechData.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess the data: Combine 'hate_speech' and 'offensive_language' into one binary label\n",
        "data['label'] = data['class'].apply(lambda x: 1 if x == 0 else (2 if x == 1 else 0))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline to vectorize the text data and train a logistic regression model\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('clf', LogisticRegression(solver='lbfgs', max_iter=1000))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Function to predict and raise a flag if the rating exceeds 0.68\n",
        "def predict_hate_speech(text):\n",
        "    prob = pipeline.predict_proba([text])[0]\n",
        "    rating = max(prob)\n",
        "    prediction = pipeline.predict([text])[0]\n",
        "    if rating > 0.68:\n",
        "        flag = True\n",
        "    else:\n",
        "        flag = False\n",
        "    return {'prediction': prediction, 'rating': rating, 'flag': flag}\n",
        "\n",
        "# Example usage\n",
        "example_text = \"I will kill you\"\n",
        "result = predict_hate_speech(example_text)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/HateSpeechData.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess the data: Combine 'hate_speech' and 'offensive_language' into one binary label\n",
        "data['label'] = data['class'].apply(lambda x: 1 if x == 0 else (2 if x == 1 else 0))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline to vectorize the text data and train a logistic regression model\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('clf', LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr'))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = pipeline.predict(X_test)\n",
        "y_prob = pipeline.predict_proba(X_test)\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1], pos_label=1)\n",
        "roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "print(f'ROC AUC: {roc_auc}')\n",
        "\n",
        "# Find the optimal threshold\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f'Optimal Threshold: {optimal_threshold}')\n",
        "\n",
        "# Function to predict and raise a flag if the rating exceeds the optimal threshold\n",
        "def predict_hate_speech(text):\n",
        "    prob = pipeline.predict_proba([text])[0]\n",
        "    rating = max(prob)\n",
        "    prediction = pipeline.predict([text])[0]\n",
        "    if rating > optimal_threshold:\n",
        "        flag = True\n",
        "    else:\n",
        "        flag = False\n",
        "    return {'prediction': prediction, 'rating': rating, 'flag': flag}\n",
        "\n",
        "# Example usage\n",
        "example_text = \"i will kill you\"\n",
        "result = predict_hate_speech(example_text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGoE9Qxe8BGj",
        "outputId": "562ebd10-4a39-4843-a2a9-f538a9c16f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8838006858987291\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.77      0.80       835\n",
            "           1       0.51      0.13      0.21       290\n",
            "           2       0.90      0.97      0.93      3832\n",
            "\n",
            "    accuracy                           0.88      4957\n",
            "   macro avg       0.75      0.62      0.65      4957\n",
            "weighted avg       0.87      0.88      0.87      4957\n",
            "\n",
            "ROC AUC: 0.9284780163501455\n",
            "Optimal Threshold: 0.05850829165662622\n",
            "{'prediction': 1, 'rating': 0.5421227134261576, 'flag': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_curve\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/HateSpeechData.csv'  # Update with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess the data\n",
        "data['label'] = data['class'].apply(lambda x: 1 if x == 0 else (2 if x == 1 else 0))\n",
        "texts = data['tweet'].values\n",
        "labels = data['label'].values\n",
        "\n",
        "# Tokenize and pad the sequences\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Embedding(max_words, 128, input_length=max_len),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')  # 3 classes: hate speech, offensive language, neither\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(y_test, y_pred_classes, target_names=['neither', 'hate_speech', 'offensive_language'])\n",
        "print(report)\n",
        "\n",
        "# Determine the optimal threshold using ROC curve\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresholds = {}\n",
        "\n",
        "for i in range(3):\n",
        "    fpr[i], tpr[i], thresholds[i] = roc_curve(y_test, y_pred[:, i], pos_label=i)\n",
        "\n",
        "# Find the optimal threshold for each class\n",
        "optimal_thresholds = {}\n",
        "for i in range(3):\n",
        "    optimal_idx = np.argmax(tpr[i] - fpr[i])\n",
        "    optimal_thresholds[i] = thresholds[i][optimal_idx]\n",
        "\n",
        "optimal_threshold = max(optimal_thresholds.values())\n",
        "print(f'Optimal Threshold: {optimal_threshold}')\n",
        "\n",
        "# Function to predict and raise a flag if the rating exceeds the optimal threshold\n",
        "def predict_content(text, threshold=optimal_threshold):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "    prob = model.predict(padded_sequence)[0]\n",
        "    rating = max(prob)\n",
        "    prediction = np.argmax(prob)\n",
        "    if rating > threshold:\n",
        "        flag = True\n",
        "    else:\n",
        "        flag = False\n",
        "    return {'prediction': prediction, 'rating': rating, 'flag': flag}\n",
        "\n",
        "# Example usage\n",
        "example_text = \"This is an example tweet.\"\n",
        "result = predict_content(example_text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMFpmIig-MMg",
        "outputId": "a5aff727-fb2f-411d-d7e6-892e471f6fd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "496/496 [==============================] - 33s 48ms/step - loss: 0.4379 - accuracy: 0.8520 - val_loss: 0.2868 - val_accuracy: 0.9070\n",
            "Epoch 2/30\n",
            "496/496 [==============================] - 12s 25ms/step - loss: 0.2570 - accuracy: 0.9183 - val_loss: 0.2903 - val_accuracy: 0.9024\n",
            "Epoch 3/30\n",
            "496/496 [==============================] - 12s 25ms/step - loss: 0.1706 - accuracy: 0.9443 - val_loss: 0.3723 - val_accuracy: 0.8886\n",
            "Epoch 4/30\n",
            "496/496 [==============================] - 12s 24ms/step - loss: 0.1217 - accuracy: 0.9610 - val_loss: 0.4103 - val_accuracy: 0.8986\n",
            "Epoch 5/30\n",
            "496/496 [==============================] - 12s 25ms/step - loss: 0.0908 - accuracy: 0.9710 - val_loss: 0.4556 - val_accuracy: 0.8838\n",
            "Epoch 6/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0673 - accuracy: 0.9775 - val_loss: 0.6192 - val_accuracy: 0.8843\n",
            "Epoch 7/30\n",
            "496/496 [==============================] - 12s 24ms/step - loss: 0.0496 - accuracy: 0.9837 - val_loss: 0.6928 - val_accuracy: 0.8850\n",
            "Epoch 8/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0418 - accuracy: 0.9866 - val_loss: 0.6311 - val_accuracy: 0.8780\n",
            "Epoch 9/30\n",
            "496/496 [==============================] - 12s 25ms/step - loss: 0.0333 - accuracy: 0.9892 - val_loss: 0.7456 - val_accuracy: 0.8797\n",
            "Epoch 10/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0305 - accuracy: 0.9897 - val_loss: 0.7784 - val_accuracy: 0.8732\n",
            "Epoch 11/30\n",
            "496/496 [==============================] - 11s 22ms/step - loss: 0.0211 - accuracy: 0.9926 - val_loss: 0.9559 - val_accuracy: 0.8744\n",
            "Epoch 12/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.9635 - val_accuracy: 0.8649\n",
            "Epoch 13/30\n",
            "496/496 [==============================] - 12s 23ms/step - loss: 0.0230 - accuracy: 0.9926 - val_loss: 1.0080 - val_accuracy: 0.8727\n",
            "Epoch 14/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0210 - accuracy: 0.9933 - val_loss: 0.8234 - val_accuracy: 0.8686\n",
            "Epoch 15/30\n",
            "496/496 [==============================] - 12s 24ms/step - loss: 0.0143 - accuracy: 0.9951 - val_loss: 1.0799 - val_accuracy: 0.8719\n",
            "Epoch 16/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0128 - accuracy: 0.9951 - val_loss: 1.1665 - val_accuracy: 0.8717\n",
            "Epoch 17/30\n",
            "496/496 [==============================] - 12s 23ms/step - loss: 0.0109 - accuracy: 0.9961 - val_loss: 1.1045 - val_accuracy: 0.8737\n",
            "Epoch 18/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.9422 - val_accuracy: 0.8772\n",
            "Epoch 19/30\n",
            "496/496 [==============================] - 11s 22ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 1.0240 - val_accuracy: 0.8636\n",
            "Epoch 20/30\n",
            "496/496 [==============================] - 11s 22ms/step - loss: 0.0129 - accuracy: 0.9962 - val_loss: 1.1408 - val_accuracy: 0.8712\n",
            "Epoch 21/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0095 - accuracy: 0.9967 - val_loss: 1.2271 - val_accuracy: 0.8691\n",
            "Epoch 22/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0101 - accuracy: 0.9963 - val_loss: 1.0636 - val_accuracy: 0.8679\n",
            "Epoch 23/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 1.2477 - val_accuracy: 0.8606\n",
            "Epoch 24/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0116 - accuracy: 0.9959 - val_loss: 1.1663 - val_accuracy: 0.8694\n",
            "Epoch 25/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 1.2184 - val_accuracy: 0.8601\n",
            "Epoch 26/30\n",
            "496/496 [==============================] - 11s 22ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 1.1979 - val_accuracy: 0.8684\n",
            "Epoch 27/30\n",
            "496/496 [==============================] - 11s 22ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 1.1998 - val_accuracy: 0.8691\n",
            "Epoch 28/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0063 - accuracy: 0.9974 - val_loss: 1.3310 - val_accuracy: 0.8699\n",
            "Epoch 29/30\n",
            "496/496 [==============================] - 11s 22ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 1.2798 - val_accuracy: 0.8565\n",
            "Epoch 30/30\n",
            "496/496 [==============================] - 11s 23ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 1.5637 - val_accuracy: 0.8664\n",
            "155/155 [==============================] - 1s 9ms/step - loss: 1.5557 - accuracy: 0.8612\n",
            "Loss: 1.5556929111480713\n",
            "Accuracy: 0.8612063527107239\n",
            "155/155 [==============================] - 3s 8ms/step\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "           neither       0.85      0.71      0.77       835\n",
            "       hate_speech       0.30      0.27      0.29       290\n",
            "offensive_language       0.90      0.94      0.92      3832\n",
            "\n",
            "          accuracy                           0.86      4957\n",
            "         macro avg       0.68      0.64      0.66      4957\n",
            "      weighted avg       0.86      0.86      0.86      4957\n",
            "\n",
            "Optimal Threshold: 0.9999967813491821\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "{'prediction': 1, 'rating': 0.9991097, 'flag': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"I like you\"\n",
        "result = predict_content(example_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOxKD9qdon_l",
        "outputId": "4c6bc06d-d407-4791-df74-da74ea552296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "{'prediction': 0, 'rating': 0.37846184, 'flag': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"I dont like you\"\n",
        "result = predict_content(example_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uHDALPeotSJ",
        "outputId": "28a89743-bd87-49ad-f051-814540ea7071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "{'prediction': 0, 'rating': 0.6263757, 'flag': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"I will kill you\"\n",
        "result = predict_content(example_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2jMsa0Fox4I",
        "outputId": "87cddbf8-ff3a-4e83-9bcd-55f7dd9db960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "{'prediction': 1, 'rating': 0.5329239, 'flag': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"I will fuck you\"\n",
        "result = predict_content(example_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "1KiUJIxtBXdu",
        "outputId": "58ce2acf-8a42-4b35-884c-3d2303f3576b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'predict_content' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-20da7acb707e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I will fuck you\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_content' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"I will fuck you up\"\n",
        "result = predict_content(example_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqyfC48_o2Eh",
        "outputId": "b01caabb-57e8-41d4-dad7-de2ca00cb335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "{'prediction': 2, 'rating': 0.93917227, 'flag': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"you are so fat\"\n",
        "result = predict_content(example_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj_v6hbxpHQ3",
        "outputId": "1bfe6667-31d0-45ab-e397-6f54bfae0536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n",
            "{'prediction': 1, 'rating': 0.5573462, 'flag': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"just go and jump off a cliff you fat fucker\"\n",
        "result = predict_content(example_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqdbS8qqpUMY",
        "outputId": "6a4f4f7e-a8f5-4cac-a608-217efe5b9a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "{'prediction': 2, 'rating': 0.98391956, 'flag': True}\n"
          ]
        }
      ]
    }
  ]
}